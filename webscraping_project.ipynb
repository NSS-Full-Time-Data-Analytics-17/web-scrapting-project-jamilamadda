{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "122f4291-52db-4755-abb8-ed2f4659dd8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as BS\n",
    "import requests\n",
    "import random\n",
    "import traceback\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0072fe4b-a95d-49d4-aa38-c7a29140d5cc",
   "metadata": {},
   "source": [
    "go to (https://httpbin.org/ip) to see ur corrent ip address"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6ef408-4d26-42a8-9c7d-7428320eaa77",
   "metadata": {},
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as BS\n",
    "\n",
    "def get_free_proxies():\n",
    "    url = 'https://free-proxy-list.net/'  # Corrected URL\n",
    "    response = requests.get(url)\n",
    "    soup = BS(response.content, 'html.parser')\n",
    "    \n",
    "    # Check if the table exists before trying to access it\n",
    "    proxy_table = soup.find(\"table\", attrs={\"class\": \"table table-striped table-bordered\"})  # Updated selector\n",
    "    \n",
    "    if proxy_table is None:\n",
    "        print(\"Could not find the proxy table. The website structure might have changed.\")\n",
    "        return []\n",
    "        \n",
    "    proxies = []\n",
    "    # Only proceed if we found the table\n",
    "    for row in proxy_table.find_all(\"tr\")[1:]:\n",
    "        tds = row.find_all(\"td\")\n",
    "        try:\n",
    "            ip = tds[0].text.strip()\n",
    "            port = tds[1].text.strip()\n",
    "            proxies.append(str(ip) + \":\" + str(port))\n",
    "        except IndexError:\n",
    "            continue\n",
    "\n",
    "    return proxies\n",
    "\n",
    "print(get_free_proxies())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc63d2a-ae16-4ca2-b333-72a9b47b3a09",
   "metadata": {},
   "source": [
    "url=\"https://httpbin.org/ip\"\n",
    "proxies=get_free_proxies()\n",
    "\n",
    "for i in range(len(proxies)):\n",
    "    print(\"Request Number : \" + str(i+1))\n",
    "    proxy=proxies[i]\n",
    "    try:\n",
    "        response=requests.get(url,proxies={\"http\":proxy})\n",
    "        print(response.json())\n",
    "    except:\n",
    "        print(\"Not Available\")\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7524e660-6c09-4e24-a73b-c9831e5e507a",
   "metadata": {},
   "source": [
    "def scrape_with_proxy(target_url, proxy_list, retries=5):\n",
    "    random.shuffle(proxy_list) # Shuffle the list to ensure randomness\n",
    "    \n",
    "    for i in range(retries):\n",
    "        proxy = proxy_list[i % len(proxy_list)] # Cycle through the list\n",
    "        proxies_dict = {\"http\": proxy, \"https\": proxy}\n",
    "        print(f\"Attempt {i+1}: Trying with proxy {proxy}\")\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(target_url, proxies=proxies_dict, timeout=8)\n",
    "            # Check if the request was successful\n",
    "            if response.status_code == 200:\n",
    "                print(f\"Success! Scraped {target_url} with proxy {proxy}\")\n",
    "                return response\n",
    "            else:\n",
    "                print(f\"Proxy {proxy} returned status code {response.status_code}. Retrying...\")\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Proxy {proxy} failed to connect: {e}. Retrying...\")\n",
    "\n",
    "    print(f\"Failed to scrape {target_url} after {retries} attempts.\")\n",
    "    return None # Return None if all retries fail\n",
    "\n",
    "# --- Example Usage ---\n",
    "all_proxies = get_free_proxies()\n",
    "if all_proxies:\n",
    "    target_site = \"https://en.wikipedia.org/wiki/List_of_largest_companies_by_revenue\" # A safe site for testing\n",
    "    successful_response = scrape_with_proxy(target_site, all_proxies, retries=10)\n",
    "\n",
    "    if successful_response:\n",
    "        print(\"\\n--- Snippet of the successfully scraped content: ---\")\n",
    "        print(successful_response.text[:300]) # Print the first 300 characters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30abcfab-7a2e-4ec8-9d00-b30912529f51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "URL='https://realpython.github.io/fake-jobs/'\n",
    "headers={'User_Agent':'Mozilla/5.0(windows NT 10.0; Win64; x64'}\n",
    "response=requests.get(URL,headers=headers)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea251fa8-6b49-4d77-854b-ae6574a1d797",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as BS\n",
    "soup=BS(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47fa1bf3-679b-4faa-a2f7-2010a0764fab",
   "metadata": {},
   "source": [
    "1.a. Use the .find method to find the tag containing the first job title (\"Senior Python Developer\"). Hint: can you find a tag type and/or a class that could be helpful for extracting this information? Extract the text from this title.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "181a4266-016c-41ec-a40f-94dfacb186aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Senior Python Developer'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.findAll('h2',class_='title is-5')[0].text.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b64b24-2b81-4cfc-90bf-df3e11ab86bb",
   "metadata": {},
   "source": [
    "1.b. Now, use what you did for the first title, but extract the job title for all jobs on this page. Store the results in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87e04b85-5ee6-4d18-a5bc-4193b5bfd596",
   "metadata": {},
   "outputs": [],
   "source": [
    "titles=soup.findAll('h2',class_='title is-5')\n",
    "title=[x.text.strip() for x in titles]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec2c30d-5caf-46ef-9962-206a42bbfb57",
   "metadata": {},
   "source": [
    "1.c. Finally, extract the companies, locations, and posting dates for each job. For example, the first job has a company of \"Payne, Roberts and Davis\", a location of \"Stewartbury, AA\", and a posting date of \"2021-04-08\". Ensure that the text that you extract is clean, meaning no extra spaces or other characters at the beginning or end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a65c137a-d3b9-4ca0-a727-ce54092bb67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "companies=soup.findAll('h3',class_='subtitle is-6 company')\n",
    "company=[x.text.strip() for x in companies]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "876e1058-5c4b-4d64-96c3-265caedd6bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "locations=soup.findAll('p',class_='location')\n",
    "location=[x.text.strip() for x in locations]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4615ce33-3bc4-4970-b9f2-a77ba0a676bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "posting_dates=soup.findAll('time',datetime='2021-04-08')\n",
    "posting_date=[x.text.strip() for x in posting_dates]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "31f8f03c-69c3-4d71-b317-73620c27a9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_list=list(zip(title,company,location,posting_date))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7a84ef99-4397-470a-9ffe-b87daca18cd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>company</th>\n",
       "      <th>location</th>\n",
       "      <th>posting_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Senior Python Developer</td>\n",
       "      <td>Payne, Roberts and Davis</td>\n",
       "      <td>Stewartbury, AA</td>\n",
       "      <td>2021-04-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Energy engineer</td>\n",
       "      <td>Vasquez-Davidson</td>\n",
       "      <td>Christopherville, AA</td>\n",
       "      <td>2021-04-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Legal executive</td>\n",
       "      <td>Jackson, Chambers and Levy</td>\n",
       "      <td>Port Ericaburgh, AA</td>\n",
       "      <td>2021-04-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Fitness centre manager</td>\n",
       "      <td>Savage-Bradley</td>\n",
       "      <td>East Seanview, AP</td>\n",
       "      <td>2021-04-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Product manager</td>\n",
       "      <td>Ramirez Inc</td>\n",
       "      <td>North Jamieview, AP</td>\n",
       "      <td>2021-04-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Museum/gallery exhibitions officer</td>\n",
       "      <td>Nguyen, Yoder and Petty</td>\n",
       "      <td>Lake Abigail, AE</td>\n",
       "      <td>2021-04-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Radiographer, diagnostic</td>\n",
       "      <td>Holder LLC</td>\n",
       "      <td>Jacobshire, AP</td>\n",
       "      <td>2021-04-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Database administrator</td>\n",
       "      <td>Yates-Ferguson</td>\n",
       "      <td>Port Susan, AE</td>\n",
       "      <td>2021-04-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Furniture designer</td>\n",
       "      <td>Ortega-Lawrence</td>\n",
       "      <td>North Tiffany, AA</td>\n",
       "      <td>2021-04-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Ship broker</td>\n",
       "      <td>Fuentes, Walls and Castro</td>\n",
       "      <td>Michelleville, AP</td>\n",
       "      <td>2021-04-08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 title                     company  \\\n",
       "0              Senior Python Developer    Payne, Roberts and Davis   \n",
       "1                      Energy engineer            Vasquez-Davidson   \n",
       "2                      Legal executive  Jackson, Chambers and Levy   \n",
       "3               Fitness centre manager              Savage-Bradley   \n",
       "4                      Product manager                 Ramirez Inc   \n",
       "..                                 ...                         ...   \n",
       "95  Museum/gallery exhibitions officer     Nguyen, Yoder and Petty   \n",
       "96            Radiographer, diagnostic                  Holder LLC   \n",
       "97              Database administrator              Yates-Ferguson   \n",
       "98                  Furniture designer             Ortega-Lawrence   \n",
       "99                         Ship broker   Fuentes, Walls and Castro   \n",
       "\n",
       "                location posting_date  \n",
       "0        Stewartbury, AA   2021-04-08  \n",
       "1   Christopherville, AA   2021-04-08  \n",
       "2    Port Ericaburgh, AA   2021-04-08  \n",
       "3      East Seanview, AP   2021-04-08  \n",
       "4    North Jamieview, AP   2021-04-08  \n",
       "..                   ...          ...  \n",
       "95      Lake Abigail, AE   2021-04-08  \n",
       "96        Jacobshire, AP   2021-04-08  \n",
       "97        Port Susan, AE   2021-04-08  \n",
       "98     North Tiffany, AA   2021-04-08  \n",
       "99     Michelleville, AP   2021-04-08  \n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(combined_list,columns=['title', 'company', 'location', 'posting_date'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ac54b6-6706-413e-bea0-f8805bb99358",
   "metadata": {},
   "source": [
    "2.a Next, add a column that contains the url for the \"Apply\" button.First, use the BeautifulSoup find_all method to extract the urls.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "24341c08-82da-43a1-a8de-a3ad80a45f6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>company</th>\n",
       "      <th>location</th>\n",
       "      <th>posting_date</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Senior Python Developer</td>\n",
       "      <td>Payne, Roberts and Davis</td>\n",
       "      <td>Stewartbury, AA</td>\n",
       "      <td>2021-04-08</td>\n",
       "      <td>https://realpython.github.io/fake-jobs/jobs/se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Energy engineer</td>\n",
       "      <td>Vasquez-Davidson</td>\n",
       "      <td>Christopherville, AA</td>\n",
       "      <td>2021-04-08</td>\n",
       "      <td>https://realpython.github.io/fake-jobs/jobs/en...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Legal executive</td>\n",
       "      <td>Jackson, Chambers and Levy</td>\n",
       "      <td>Port Ericaburgh, AA</td>\n",
       "      <td>2021-04-08</td>\n",
       "      <td>https://realpython.github.io/fake-jobs/jobs/le...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Fitness centre manager</td>\n",
       "      <td>Savage-Bradley</td>\n",
       "      <td>East Seanview, AP</td>\n",
       "      <td>2021-04-08</td>\n",
       "      <td>https://realpython.github.io/fake-jobs/jobs/fi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Product manager</td>\n",
       "      <td>Ramirez Inc</td>\n",
       "      <td>North Jamieview, AP</td>\n",
       "      <td>2021-04-08</td>\n",
       "      <td>https://realpython.github.io/fake-jobs/jobs/pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Museum/gallery exhibitions officer</td>\n",
       "      <td>Nguyen, Yoder and Petty</td>\n",
       "      <td>Lake Abigail, AE</td>\n",
       "      <td>2021-04-08</td>\n",
       "      <td>https://realpython.github.io/fake-jobs/jobs/mu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Radiographer, diagnostic</td>\n",
       "      <td>Holder LLC</td>\n",
       "      <td>Jacobshire, AP</td>\n",
       "      <td>2021-04-08</td>\n",
       "      <td>https://realpython.github.io/fake-jobs/jobs/ra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Database administrator</td>\n",
       "      <td>Yates-Ferguson</td>\n",
       "      <td>Port Susan, AE</td>\n",
       "      <td>2021-04-08</td>\n",
       "      <td>https://realpython.github.io/fake-jobs/jobs/da...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Furniture designer</td>\n",
       "      <td>Ortega-Lawrence</td>\n",
       "      <td>North Tiffany, AA</td>\n",
       "      <td>2021-04-08</td>\n",
       "      <td>https://realpython.github.io/fake-jobs/jobs/fu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Ship broker</td>\n",
       "      <td>Fuentes, Walls and Castro</td>\n",
       "      <td>Michelleville, AP</td>\n",
       "      <td>2021-04-08</td>\n",
       "      <td>https://realpython.github.io/fake-jobs/jobs/sh...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 title                     company  \\\n",
       "0              Senior Python Developer    Payne, Roberts and Davis   \n",
       "1                      Energy engineer            Vasquez-Davidson   \n",
       "2                      Legal executive  Jackson, Chambers and Levy   \n",
       "3               Fitness centre manager              Savage-Bradley   \n",
       "4                      Product manager                 Ramirez Inc   \n",
       "..                                 ...                         ...   \n",
       "95  Museum/gallery exhibitions officer     Nguyen, Yoder and Petty   \n",
       "96            Radiographer, diagnostic                  Holder LLC   \n",
       "97              Database administrator              Yates-Ferguson   \n",
       "98                  Furniture designer             Ortega-Lawrence   \n",
       "99                         Ship broker   Fuentes, Walls and Castro   \n",
       "\n",
       "                location posting_date  \\\n",
       "0        Stewartbury, AA   2021-04-08   \n",
       "1   Christopherville, AA   2021-04-08   \n",
       "2    Port Ericaburgh, AA   2021-04-08   \n",
       "3      East Seanview, AP   2021-04-08   \n",
       "4    North Jamieview, AP   2021-04-08   \n",
       "..                   ...          ...   \n",
       "95      Lake Abigail, AE   2021-04-08   \n",
       "96        Jacobshire, AP   2021-04-08   \n",
       "97        Port Susan, AE   2021-04-08   \n",
       "98     North Tiffany, AA   2021-04-08   \n",
       "99     Michelleville, AP   2021-04-08   \n",
       "\n",
       "                                                  url  \n",
       "0   https://realpython.github.io/fake-jobs/jobs/se...  \n",
       "1   https://realpython.github.io/fake-jobs/jobs/en...  \n",
       "2   https://realpython.github.io/fake-jobs/jobs/le...  \n",
       "3   https://realpython.github.io/fake-jobs/jobs/fi...  \n",
       "4   https://realpython.github.io/fake-jobs/jobs/pr...  \n",
       "..                                                ...  \n",
       "95  https://realpython.github.io/fake-jobs/jobs/mu...  \n",
       "96  https://realpython.github.io/fake-jobs/jobs/ra...  \n",
       "97  https://realpython.github.io/fake-jobs/jobs/da...  \n",
       "98  https://realpython.github.io/fake-jobs/jobs/fu...  \n",
       "99  https://realpython.github.io/fake-jobs/jobs/sh...  \n",
       "\n",
       "[100 rows x 5 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urls=soup.findAll('a',class_='card-footer-item')\n",
    "url=[x['href'] for x in urls]\n",
    "url=url[1::2]\n",
    "df['url']=url\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6693761f-fcda-46ac-8d70-cc8adad9f242",
   "metadata": {},
   "source": [
    "2.b. Next, get those same urls in a different way. Examine the urls and see if you can spot the pattern of how they are constructed. Then, build the url using the elements you have already extracted. Ensure that the urls that you created match those that you extracted using BeautifulSoup. Warning: You will need to do some string cleaning and prep in constructing the urls this way. For example, look carefully at the urls for the \"Software Engineer (Python)\" job and the \"Scientist, research (maths)\" job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9c5083c0-f200-44eb-917d-ecca62e583bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = 'https://realpython.github.io/fake-jobs/'\n",
    "df['url1'] = (base_url + df['title'].str.lower().str.replace(' ', '-') + '-' + df.index.astype(str)+ '.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141e38cf-e45e-4082-b6b6-46968e3f9330",
   "metadata": {},
   "source": [
    "3. Finally, we want to get the job description text for each job.  \n",
    "    a. Start by looking at the page for the first job, https://realpython.github.io/fake-jobs/jobs/senior-python-developer-0.html. Using BeautifulSoup, extract the job description paragraph. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3dc9b84f-3f43-41ff-b49a-526256f6ba2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "url2='https://realpython.github.io/fake-jobs/jobs/senior-python-developer-0.html'\n",
    "headers={'User_Agent':'Mozilla/5.0(windows NT 10.0; Win64; x64'}\n",
    "response=requests.get(url2,headers=headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bb4e6db9-df83-4480-9a85-9f333f9c6717",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Professional asset web application environmentally friendly detail-oriented asset. Coordinate educational dashboard agile employ growth opportunity. Company programs CSS explore role. Html educational grit web application. Oversea SCRUM talented support. Web Application fast-growing communities inclusive programs job CSS. Css discussions growth opportunity explore open-minded oversee. Css Python environmentally friendly collaborate inclusive role. Django no experience oversee dashboard environmentally friendly willing to learn programs. Programs open-minded programs asset.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup=BS(response.text)\n",
    "soup.findAll('p')[1].text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e546a3a4-f1f7-477a-9e78-ba73aed1ba44",
   "metadata": {},
   "source": [
    "3b. We want to be able to do this for all pages. Write a function which takes as input a url and returns the description text on that page. For example, if you input \"https://realpython.github.io/fake-jobs/jobs/television-floor-manager-8.html\" into your function, it should return the string \"At be than always different American address. Former claim chance prevent why measure too. Almost before some military outside baby interview. Face top individual win suddenly. Parent do ten after those scientist. Medical effort assume teacher wall. Significant his himself clearly very. Expert stop area along individual. Three own bank recognize special good along.\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bd7d9b01-330c-467c-89a6-a1b19b02335f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_description(url):\n",
    "    try:\n",
    "        response=requests.get(url)\n",
    "        soup=BS(response.text)\n",
    "        paragraph=soup.findAll('p')[1].text\n",
    "        return paragraph\n",
    "    except:\n",
    "        return \"\"\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1d1ec8-7584-48e0-8ebd-0823a78af1fe",
   "metadata": {},
   "source": [
    "Q3.c c. Use the .apply method on the url column you created above to retrieve the description text for all of the jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "63040778-6e39-4f42-9f3a-3cd491589ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['description']=df['url'].apply(print_description)   \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e887db67-f443-4f20-831d-c05169468ff7",
   "metadata": {},
   "source": [
    "BONUS\n",
    "Q1. Navigate to https://www.billboard.com/charts/hot-100/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ae494e6e-5d09-4d99-b661-383d5695bb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "URL='https://www.billboard.com/charts/hot-100/'\n",
    "headers=headers={'User_Agent':'Mozilla/5.0(windows NT 10.0; Win64; x64'}\n",
    "response=requests.get(URL,headers=headers)\n",
    "soup=BS(response.content,'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01be1df-8734-4626-a48f-f8adb10484d9",
   "metadata": {},
   "source": [
    "Using BeautifulSoup, extract out the This Week's artist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5ff050bc-84d8-4ebc-abe5-eff5d112795b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Taylor Swift',\n",
       " 'HUNTR/X: EJAE, Audrey Nuna & REI AMI',\n",
       " 'Alex Warren',\n",
       " 'Olivia Dean',\n",
       " 'Taylor Swift',\n",
       " 'Leon Thomas',\n",
       " 'Justin Bieber',\n",
       " 'Mariah Carey',\n",
       " 'Kehlani',\n",
       " 'Morgan Wallen',\n",
       " 'Wham!',\n",
       " 'sombr',\n",
       " 'Brenda Lee',\n",
       " 'Bobby Helms',\n",
       " 'Morgan Wallen Featuring Tate McRae',\n",
       " 'Ella Langley',\n",
       " 'Chris Brown Featuring Bryson Tiller',\n",
       " 'Olivia Dean',\n",
       " 'Ariana Grande',\n",
       " 'Sabrina Carpenter',\n",
       " 'Saja Boys: Andrew Choi, Neckwav, Danny Chung, Kevin Woo & samUIL Lee',\n",
       " 'Sabrina Carpenter',\n",
       " 'Ravyn Lenae',\n",
       " 'Saja Boys: Andrew Choi, Neckwav, Danny Chung, Kevin Woo & samUIL Lee',\n",
       " 'Cody Johnson',\n",
       " 'Tate McRae',\n",
       " 'Kelly Clarkson',\n",
       " 'Taylor Swift',\n",
       " 'Nat \"King\" Cole',\n",
       " 'Andy Williams',\n",
       " 'KATSEYE',\n",
       " 'Michael Buble',\n",
       " 'Riley Green Featuring Ella Langley',\n",
       " 'Dean Martin',\n",
       " 'NF',\n",
       " 'HUNTR/X: EJAE, Audrey Nuna & REI AMI',\n",
       " 'The Ronettes',\n",
       " 'Megan Moroney',\n",
       " 'Lainey Wilson',\n",
       " 'Burl Ives',\n",
       " 'Disco Lines & Tinashe',\n",
       " 'HUNTR/X: EJAE, Audrey Nuna & REI AMI',\n",
       " 'Russell Dickerson',\n",
       " 'Taylor Swift',\n",
       " 'Gunna Featuring Burna Boy',\n",
       " 'Taylor Swift',\n",
       " 'RAYE',\n",
       " 'Darlene Love',\n",
       " 'Jose Feliciano',\n",
       " 'Morgan Wallen',\n",
       " 'BigXthaPlug Featuring Ella Langley',\n",
       " 'Justin Bieber',\n",
       " 'Tame Impala',\n",
       " 'Mariah the Scientist & Kali Uchis',\n",
       " 'Olivia Dean',\n",
       " 'Myles Smith',\n",
       " 'Hudson Westbrook',\n",
       " 'Summer Walker & Mariah the Scientist',\n",
       " 'Chase Matthew',\n",
       " 'Summer Walker, Latto & Doja Cat',\n",
       " 'YoungBoy Never Broke Again',\n",
       " 'NF & MGK',\n",
       " 'Max McNown',\n",
       " 'Taylor Swift Featuring Sabrina Carpenter',\n",
       " 'Luke Combs',\n",
       " 'Cardi B',\n",
       " 'Doja Cat',\n",
       " 'Summer Walker & Chris Brown',\n",
       " 'Summer Walker',\n",
       " 'NF With James Arthur',\n",
       " 'Gavin Adcock',\n",
       " 'sombr',\n",
       " 'Tyler, the Creator',\n",
       " 'Metro Boomin, Quavo, Breskii & YKNIECE',\n",
       " 'Cody Johnson',\n",
       " 'Taylor Swift',\n",
       " 'Taylor Swift',\n",
       " 'Taylor Swift',\n",
       " 'Summer Walker',\n",
       " 'Summer Walker & Anderson .Paak',\n",
       " 'Taylor Swift',\n",
       " 'Olivia Dean',\n",
       " 'Parker McCollum',\n",
       " 'G Herbo',\n",
       " 'NF',\n",
       " 'NF',\n",
       " 'Cardi B Featuring Kehlani',\n",
       " 'Coldplay',\n",
       " 'Taylor Swift',\n",
       " 'Lady Gaga',\n",
       " 'Sabrina Carpenter',\n",
       " 'The Marias',\n",
       " 'Ed Sheeran',\n",
       " 'Luke Combs',\n",
       " 'Shaboozey & Jelly Roll',\n",
       " 'Summer Walker, Glorilla, Sexyy Red & Monaleo',\n",
       " 'Olivia Dean',\n",
       " 'Blake Roman',\n",
       " 'HARDY',\n",
       " 'Jason Aldean']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "artists= soup.select('span.c-label.a-no-trucate')\n",
    "artist=[x.text.strip() for x in artists]\n",
    "artist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f3ce83-7422-4efa-934b-8b1da7b04b51",
   "metadata": {},
   "source": [
    "extract this week's song."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7c3ed9aa-6cf4-4636-b075-b35f93ebd94a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The Fate Of Ophelia',\n",
       " 'Golden',\n",
       " 'Ordinary',\n",
       " 'Man I Need',\n",
       " 'Opalite',\n",
       " 'Mutt',\n",
       " 'Daisies',\n",
       " 'All I Want For Christmas Is You',\n",
       " 'Folded',\n",
       " 'I Got Better',\n",
       " 'Last Christmas',\n",
       " 'Back To Friends',\n",
       " \"Rockin' Around The Christmas Tree\",\n",
       " 'Jingle Bell Rock',\n",
       " 'What I Want',\n",
       " \"Choosin' Texas\",\n",
       " 'It Depends',\n",
       " 'So Easy (To Fall In Love)',\n",
       " 'Santa Tell Me',\n",
       " 'Manchild',\n",
       " 'Soda Pop',\n",
       " 'Tears',\n",
       " 'Love Me Not',\n",
       " 'Your Idol',\n",
       " \"Travelin' Soldier\",\n",
       " 'Tit For Tat',\n",
       " 'Underneath The Tree',\n",
       " 'Elizabeth Taylor',\n",
       " 'The Christmas Song (Merry Christmas To You)',\n",
       " \"It's The Most Wonderful Time Of The Year\",\n",
       " 'Gabriela',\n",
       " \"It's Beginning To Look A Lot Like Christmas\",\n",
       " \"Don't Mind If I Do\",\n",
       " 'Let It Snow! Let It Snow! Let It Snow!',\n",
       " 'Fear',\n",
       " \"How It's Done\",\n",
       " 'Sleigh Ride',\n",
       " '6 Months Later',\n",
       " 'Somewhere Over Laredo',\n",
       " 'A Holly Jolly Christmas',\n",
       " 'No Broke Boys',\n",
       " 'What It Sounds Like',\n",
       " 'Happen To Me',\n",
       " 'Wi$h Li$t',\n",
       " 'wgft',\n",
       " 'Father Figure',\n",
       " 'Where Is My Husband!',\n",
       " 'Christmas (Baby Please Come Home)',\n",
       " 'Feliz Navidad',\n",
       " '20 Cigarettes',\n",
       " 'Hell At Night',\n",
       " 'Yukon',\n",
       " 'Dracula',\n",
       " 'Is It A Crime',\n",
       " 'A Couple Minutes',\n",
       " 'Nice To Meet You',\n",
       " 'House Again',\n",
       " 'Robbed You',\n",
       " \"Darlin'\",\n",
       " 'Go Girl',\n",
       " 'Shot Callin',\n",
       " 'Who I Was',\n",
       " 'Better Me For You (Brown Eyes)',\n",
       " 'The Life Of A Showgirl',\n",
       " 'Back In The Saddle',\n",
       " 'ErrTime',\n",
       " 'Jealous Type',\n",
       " 'Baby',\n",
       " 'No',\n",
       " 'Sorry',\n",
       " 'Last One To Know',\n",
       " '12 To 12',\n",
       " 'Sugar On My Tongue',\n",
       " 'Take Me Thru Dere',\n",
       " 'The Fall',\n",
       " 'Ruin The Friendship',\n",
       " 'Wood',\n",
       " 'Actually Romantic',\n",
       " 'FMT',\n",
       " '1-800 Heartbreak',\n",
       " 'Eldest Daughter',\n",
       " 'Nice To Each Other',\n",
       " 'What Kinda Man',\n",
       " 'Went Legit',\n",
       " 'Give Me A Reason',\n",
       " 'Home',\n",
       " 'Safe',\n",
       " 'Sparks',\n",
       " 'Cancelled!',\n",
       " 'The Dead Dance',\n",
       " 'When Did You Get Hot?',\n",
       " 'Sienna',\n",
       " 'Camera',\n",
       " 'Days Like These',\n",
       " 'Amen',\n",
       " 'Baller',\n",
       " 'Let Alone The One You Love',\n",
       " \"Losin' Streak\",\n",
       " 'Favorite Country Song',\n",
       " 'How Far Does A Goodbye Go']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "songs=soup.select('h3.c-title.a-font-basic.u-letter-spacing-0010')\n",
    "song=[x.text.strip() for x in songs]\n",
    "song"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8741982-0034-4381-93a5-7b26f8418ca4",
   "metadata": {},
   "source": [
    "extract song's peak position"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166db5ad-b4c5-4c82-8bc2-4a1e98298fae",
   "metadata": {},
   "source": [
    "peak_positions=soup.select('span.c-label.u-font-family-secondary\\\\@mobile-max.u-font-family-basic\\\\@tablet')\n",
    "peak_positions=soup.select('li.lrv-u-flex.lrv-u-flex-shrink-0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480a28fa-ad9a-4903-8558-c8336d339295",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
